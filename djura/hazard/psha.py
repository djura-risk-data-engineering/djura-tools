from pathlib import Path
from scipy.interpolate import interp1d
from pandas import read_csv
import numpy as np
import json


def proc_oq_hazard_curve(
    poes: list[float],
    path_hazard_results: str | Path,
    json_file: str | Path = 'hazard.json',
    haz_file_start: str = 'hazard_curve-mean'
) -> None:
    """
    Process OpenQuake hazard curve results and store them in a JSON file.

    This function processes the hazard curve files generated by OpenQuake
    after performing Probabilistic Seismic Hazard Analysis (PSHA). For each
    hazard curve file, it interpolates the intensity measure levels (IMLs)
    corresponding to specified probabilities of exceedance (poes). The
    processed data, including the site location, investigation time, hazard
    curves, and interpolated IMLs, is then saved into a JSON file.

    Parameters
    ----------
    poes : list[float]
        List of probabilities of exceedance in a given investigation time
        (`investigation_time`) for which corresponding intensity measure
        levels (IMLs) will be obtained via interpolation.
    path_hazard_results : str | Path
        Path to the directory containing the OpenQuake hazard curve results.
    json_file : str | Path, optional
        Path and name of the output JSON file where the processed data will
        be saved. By default, it is saved as 'hazard.json'.
    haz_file_start : str, optional
        Prefix for the hazard curve files to process. Only files that start
        with this prefix will be processed. By default, this is set to
        'hazard_curve-mean'.

    Returns
    -------
    None

    Notes
    -----
    The output JSON file will contain the following keys:

    - `investigation_time`: The investigation time used in the PSHA
    (extracted from the hazard files).
    - `lat`: List of latitudes of the investigated sites.
    - `lon`: List of longitudes of the investigated sites.
    - `im`: List of intensity measure types (IMs) extracted from the hazard
    curve file names.
    - `cond_poes`: The provided list of probabilities of exceedance for which
    IMLs are calculated.
    - `cond_imls`: A dictionary containing interpolated IMLs corresponding to
    each IM for the given poes.
    - `hazard_curves`: A dictionary containing hazard curve data for each IM,
    including the original probabilities of exceedance and IMLs.

    Example
    -------
    To process hazard curve files in a directory and save them in a JSON file,
    run:

    >>> proc_oq_hazard_curve([0.1, 0.5], 'path/to/results', 'outputs.json')

    This will process hazard curve files from the directory `path/to/results`
    and save the results in `outputs.json`.
    """

    # Convert paths to Path objects
    path_hazard_results = Path(path_hazard_results)
    json_file = Path(json_file)

    # Initialise dictionary to store all outputs
    output_data = {
        "investigation_time": None,  # Will be set when reading files
        "lat": [],
        "lon": [],
        "im": [],
        "cond_poes": poes,
        "cond_imls": {},
        "hazard_curves": {}
    }

    # Read through each file in the outputs folder
    for file in path_hazard_results.iterdir():
        if file.name.startswith(haz_file_start):

            # Strip the IM out of the file name
            im_type = (file.stem.split('-')[2]).split('_')[0]

            # Load the results in as a dataframe
            df = read_csv(file, skiprows=1)

            # Get the column headers
            iml = list(df.columns.values)[3:]  # List of headers
            iml = [float(i[4:]) for i in iml]  # Strip out the actual IM values

            with file.open("r") as f:
                temp1 = f.readline().split(',')
                temp2 = list(filter(None, temp1))
                inv_t = float(list(filter(
                    lambda x: 'investigation_time=' in x, temp2
                ))[0].replace(" investigation_time=", ""))

                # Save inv_t once (assuming it is consistent across files)
                output_data["investigation_time"] = inv_t

            # For each of the sites investigated
            for site in np.arange(len(df)):

                # Append each site's info to the output dictionary
                output_data["lat"].append([df.lat[site]][0])
                output_data["lon"].append([df.lon[site]][0])
                output_data["im"].append(im_type)

                # Get the array of poe in inv_t and corresponding imls
                tmp1 = np.array(df.iloc[site, 3:].values)
                tmp2 = np.array(iml)
                # get rid of any infinite or nan value
                infs = np.isinf(tmp1)
                tmp1 = tmp1[~infs]
                tmp2 = tmp2[~infs]
                nans = np.isnan(tmp1)
                tmp1 = tmp1[~nans]
                tmp2 = tmp2[~nans]

                # Save hazard curve data in the dictionary
                output_data["hazard_curves"][f"{im_type}"] = {
                    "poe": tmp1.tolist(),
                    "iml": tmp2.tolist(),
                }

    # Get intensity measure levels corresponding to poes and store in
    # dictionary
    for idx, im in enumerate(output_data["hazard_curves"]):
        poe = output_data["hazard_curves"][im]["poe"]
        iml = output_data["hazard_curves"][im]["iml"]
        iml_interp = interp1d(poe, iml, kind='linear')(poes)
        output_data["cond_imls"][output_data["im"][idx]] = iml_interp.tolist()

    # Save the output dictionary as a JSON file
    if json_file:
        with open(json_file, 'w') as file:
            json.dump(output_data, file, indent=4)

    return output_data
